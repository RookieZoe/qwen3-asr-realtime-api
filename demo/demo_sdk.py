#!/usr/bin/env python3
"""
Qwen3-ASR Realtime Python Demo
ä½¿ç”¨å®˜æ–¹ DashScope SDK è¿æ¥ç§æœ‰åç«¯æœåŠ¡

åŠŸèƒ½:
- VAD æ¨¡å¼: å®æ—¶å½•éŸ³è¯†åˆ«
- Manual æ¨¡å¼: éŸ³é¢‘æ–‡ä»¶è¯†åˆ«
- æ”¯æŒå®æ—¶æ˜¾ç¤ºè¯†åˆ«ç»“æœ
"""

import argparse
import base64
import os
import signal
import sys
import time
from datetime import datetime

# å®‰è£…: pip install dashscope>=1.25.6
import dashscope
from dashscope.audio.qwen_omni import OmniRealtimeCallback, OmniRealtimeConversation
from dashscope.audio.qwen_omni.omni_realtime import TranscriptionParams

# ==================== é…ç½® ====================

# ç§æœ‰åç«¯æœåŠ¡åœ°å€ (ä¿®æ”¹ä¸ºä½ çš„æœåŠ¡åœ°å€)
DEFAULT_WS_URL = "ws://localhost:8080/api-ws/v1/realtime"

# API Key (ç§æœ‰æœåŠ¡é€šå¸¸ä¸éœ€è¦ï¼Œä½† SDK è¦æ±‚æä¾›ï¼Œå¯ä»¥éšä¾¿å¡«)
DEFAULT_API_KEY = "dummy-api-key-for-local-server"


# ==================== æ—¥å¿—é…ç½® ====================

def setup_logging():
    """é…ç½®æ—¥å¿—è¾“å‡º"""
    import logging

    logger = logging.getLogger("dashscope")
    logger.setLevel(logging.INFO)

    # æ¸…é™¤å·²æœ‰å¤„ç†å™¨
    logger.handlers = []

    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.INFO)
    formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.propagate = False
    return logger


# ==================== å›è°ƒå¤„ç† ====================


class ASRCallback(OmniRealtimeCallback):
    """å®æ—¶è¯†åˆ«å›è°ƒå¤„ç†"""

    def __init__(self):
        self.confirmed_text = ""
        self.stash_text = ""
        self.is_running = True

    def on_open(self):
        print("\nâœ… è¿æ¥æˆåŠŸ")

    def on_close(self, code, msg):
        print(f"\nâŒ è¿æ¥å…³é—­, code: {code}, msg: {msg}")
        self.is_running = False

    def on_event(self, response):
        event_type = response.get("type", "")

        # ä¼šè¯åˆ›å»º
        if event_type == "session.created":
            session_id = response.get("session", {}).get("id", "unknown")
            print(f"ğŸ“¢ ä¼šè¯åˆ›å»º: {session_id}")

        # ä¼šè¯æ›´æ–°
        elif event_type == "session.updated":
            print("ğŸ“¢ ä¼šè¯é…ç½®æ›´æ–°æˆåŠŸ")

        # è¯­éŸ³å¼€å§‹ (VAD æ¨¡å¼)
        elif event_type == "input_audio_buffer.speech_started":
            print("\nğŸ¤ [æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹]")

        # è¯­éŸ³ç»“æŸ (VAD æ¨¡å¼)
        elif event_type == "input_audio_buffer.speech_stopped":
            print("ğŸ›‘ [æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ]")

        # å®æ—¶è¯†åˆ«ç»“æœ
        elif event_type == "conversation.item.input_audio_transcription.text":
            text = response.get("text", "")
            stash = response.get("stash", "")
            language = response.get("language", "")
            emotion = response.get("emotion", "")

            # æ›´æ–°å·²ç¡®è®¤æ–‡æœ¬
            if text:
                self.confirmed_text = text
            self.stash_text = stash

            # å®æ—¶æ˜¾ç¤º
            display_text = text + stash
            print(f"\rğŸ“ è¯†åˆ«ä¸­: {display_text[:80]}...", end="", flush=True)

        # æœ€ç»ˆè¯†åˆ«ç»“æœ
        elif event_type == "conversation.item.input_audio_transcription.completed":
            transcript = response.get("transcript", "")
            language = response.get("language", "")
            emotion = response.get("emotion", "")

            self.confirmed_text += transcript
            self.stash_text = ""

            print(f"\nâœ… [è¯†åˆ«å®Œæˆ]")
            print(f"   æ–‡æœ¬: {transcript}")
            print(f"   è¯­è¨€: {language}")
            print(f"   æƒ…æ„Ÿ: {emotion}")

        # ä¼šè¯ç»“æŸ
        elif event_type == "session.finished":
            print("\nğŸ ä¼šè¯ç»“æŸ")
            self.is_running = False

        # é”™è¯¯
        elif event_type == "error":
            error = response.get("error", {})
            print(f"\nâŒ [é”™è¯¯] {error.get('message', 'Unknown error')}")


# ==================== éŸ³é¢‘å¤„ç† ====================


def list_audio_devices():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„éŸ³é¢‘è¾“å…¥è®¾å¤‡"""
    try:
        import pyaudio
    except ImportError:
        print("è¯·å…ˆå®‰è£… pyaudio: pip install pyaudio")
        return []

    audio = pyaudio.PyAudio()
    devices = []

    print("\nğŸ¤ å¯ç”¨éŸ³é¢‘è¾“å…¥è®¾å¤‡:")
    print("-" * 60)

    for i in range(audio.get_device_count()):
        info = audio.get_device_info_by_index(i)
        if int(info["maxInputChannels"]) > 0:  # ä»…æ˜¾ç¤ºè¾“å…¥è®¾å¤‡
            is_default = info.get("index") == audio.get_default_input_device_info().get("index")
            default_marker = " â­ (é»˜è®¤)" if is_default else ""
            print(f"  [{i}] {info['name']}{default_marker}")
            print(f"      é‡‡æ ·ç‡: {int(info['defaultSampleRate'])} Hz, è¾“å…¥é€šé“: {int(info['maxInputChannels'])}")
            devices.append({"index": i, "name": info["name"], "is_default": is_default})

    print("-" * 60)
    audio.terminate()
    return devices


def read_audio_chunks(file_path, chunk_size=3200):
    """æŒ‰å—è¯»å–éŸ³é¢‘æ–‡ä»¶ (3200 bytes = 0.1s PCM16/16kHz)"""
    with open(file_path, "rb") as f:
        while chunk := f.read(chunk_size):
            yield chunk


def send_audio_file(conversation, file_path, delay=0.1):
    """å‘é€éŸ³é¢‘æ–‡ä»¶"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    print(f"ğŸ“ æ­£åœ¨å¤„ç†: {file_path}")
    print(f"â±ï¸  å‘é€é—´éš”: {delay}s (æ¨¡æ‹Ÿå®æ—¶é‡‡é›†)")
    print("ğŸ›‘ æŒ‰ Ctrl+C åœæ­¢\n")

    total_bytes = 0
    start_time = time.time()

    for chunk in read_audio_chunks(file_path):
        audio_b64 = base64.b64encode(chunk).decode("ascii")
        conversation.append_audio(audio_b64)
        total_bytes += len(chunk)
        time.sleep(delay)

    elapsed = time.time() - start_time
    print(f"\nğŸ“Š å‘é€å®Œæˆ: {total_bytes} bytes in {elapsed:.1f}s")


# ==================== ä¸»ç¨‹åº ====================


def run_vad_mode(url, api_key, language="auto", device_index=None):
    """
    VAD æ¨¡å¼: å®æ—¶å½•éŸ³è¯†åˆ«

    éœ€è¦å®‰è£…: pip install pyaudio

    Args:
        url: WebSocket æœåŠ¡åœ°å€
        api_key: API Key
        language: è¯†åˆ«è¯­è¨€
        device_index: éŸ³é¢‘è¾“å…¥è®¾å¤‡ç´¢å¼• (None è¡¨ç¤ºä½¿ç”¨é»˜è®¤è®¾å¤‡)
    """
    try:
        import pyaudio
    except ImportError:
        print("è¯·å…ˆå®‰è£… pyaudio: pip install pyaudio")
        return

    print("=" * 60)
    print("ğŸ™ï¸ VAD æ¨¡å¼ - å®æ—¶å½•éŸ³è¯†åˆ«")
    print("=" * 60)
    print(f"æœåŠ¡ç«¯: {url}")
    print(f"è¯­è¨€: {language}")

    # è·å–è®¾å¤‡ä¿¡æ¯
    audio = pyaudio.PyAudio()
    if device_index is not None:
        try:
            device_info = audio.get_device_info_by_index(device_index)
            print(f"éŸ³é¢‘è®¾å¤‡: [{device_index}] {device_info['name']}")
        except Exception as e:
            print(f"âŒ æ— æ•ˆçš„è®¾å¤‡ç´¢å¼• {device_index}: {e}")
            audio.terminate()
            return
    else:
        device_info = audio.get_default_input_device_info()
        print(f"éŸ³é¢‘è®¾å¤‡: [é»˜è®¤] {device_info['name']}")

    print("æŒ‰ Ctrl+C åœæ­¢å½•éŸ³\n")

    # åˆ›å»ºå›è°ƒ
    callback = ASRCallback()

    # åˆ›å»ºä¼šè¯
    conversation = OmniRealtimeConversation(
        model="qwen3-asr-flash-realtime", url=url, callback=callback
    )

    # é…ç½®
    transcription_params = TranscriptionParams(
        language=language, sample_rate=16000, input_audio_format="pcm"
    )

    # è¿æ¥
    conversation.connect()

    conversation.update_session(
        output_modalities=["text"],
        enable_turn_detection=True,
        turn_detection_type="server_vad",
        turn_detection_threshold=0.3,
        turn_detection_silence_duration_ms=500,
        enable_input_audio_transcription=True,
        transcription_params=transcription_params,
    )

    # å½•éŸ³å‚æ•°
    CHUNK = 1024
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 16000

    # åˆå§‹åŒ–å½•éŸ³ (å¤ç”¨å·²åˆ›å»ºçš„ audio å®ä¾‹)
    stream = audio.open(
        format=FORMAT,
        channels=CHANNELS,
        rate=RATE,
        input=True,
        frames_per_buffer=CHUNK,
        input_device_index=device_index,
    )

    print("ğŸ¤ å¼€å§‹å½•éŸ³...\n")

    try:
        while callback.is_running:
            # è¯»å–éŸ³é¢‘æ•°æ®
            data = stream.read(CHUNK, exception_on_overflow=False)

            # è½¬æ¢ä¸º base64 å‘é€
            audio_b64 = base64.b64encode(data).decode("ascii")
            conversation.append_audio(audio_b64)

    except KeyboardInterrupt:
        print("\n\nğŸ›‘ åœæ­¢å½•éŸ³")
    finally:
        stream.stop_stream()
        stream.close()
        audio.terminate()

        # ç»“æŸä¼šè¯
        conversation.end_session()
        time.sleep(2)
        conversation.close()

        print(f"\n{'=' * 60}")
        print(f"ğŸ“ æœ€ç»ˆè¯†åˆ«ç»“æœ:\n{callback.confirmed_text}")
        print(f"{'=' * 60}")


def run_manual_mode(url, api_key, audio_file, language="auto", delay=0.1):
    """
    Manual æ¨¡å¼: éŸ³é¢‘æ–‡ä»¶è¯†åˆ«
    """
    print("=" * 60)
    print("ğŸ“ Manual æ¨¡å¼ - éŸ³é¢‘æ–‡ä»¶è¯†åˆ«")
    print("=" * 60)
    print(f"æœåŠ¡ç«¯: {url}")
    print(f"æ–‡ä»¶: {audio_file}")
    print(f"è¯­è¨€: {language}")
    print(f"å‘é€é—´éš”: {delay}s\n")

    # åˆ›å»ºå›è°ƒ
    callback = ASRCallback()

    # åˆ›å»ºä¼šè¯
    conversation = OmniRealtimeConversation(
        model="qwen3-asr-flash-realtime", url=url, callback=callback
    )

    # é…ç½®
    transcription_params = TranscriptionParams(
        language=language, sample_rate=16000, input_audio_format="pcm"
    )

    # è¿æ¥
    conversation.connect()

    conversation.update_session(
        output_modalities=["text"],
        enable_turn_detection=False,  # Manual æ¨¡å¼å…³é—­ VAD
        enable_input_audio_transcription=True,
        transcription_params=transcription_params,
    )

    # ç­‰å¾…ä¼šè¯é…ç½®å®Œæˆ
    time.sleep(1)

    try:
        # å‘é€éŸ³é¢‘
        send_audio_file(conversation, audio_file, delay)

        # æäº¤è¯†åˆ« (Manual æ¨¡å¼éœ€è¦)
        print("\nğŸ“¤ æäº¤è¯†åˆ«...")
        conversation.commit()

        # ç­‰å¾…è¯†åˆ«å®Œæˆ
        time.sleep(3)

        # ç»“æŸä¼šè¯
        print("ğŸ ç»“æŸä¼šè¯...")
        conversation.end_session()

        # ç­‰å¾…ç»“æœ
        timeout = 30
        start = time.time()
        while callback.is_running and time.time() - start < timeout:
            time.sleep(0.5)

    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
    finally:
        conversation.close()

        print(f"\n{'=' * 60}")
        print(f"ğŸ“ æœ€ç»ˆè¯†åˆ«ç»“æœ:\n{callback.confirmed_text}")
        print(f"{'=' * 60}")


# ==================== å‘½ä»¤è¡Œå…¥å£ ====================


def main():
    parser = argparse.ArgumentParser(
        description="Qwen3-ASR Realtime Python Demo",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åˆ—å‡ºå¯ç”¨éŸ³é¢‘è®¾å¤‡
  python demo_sdk.py --list-devices

  # VAD æ¨¡å¼ (ä½¿ç”¨é»˜è®¤éŸ³é¢‘è®¾å¤‡)
  python demo_sdk.py --mode vad --url ws://localhost:8080/api-ws/v1/realtime

  # VAD æ¨¡å¼ (æŒ‡å®šéŸ³é¢‘è®¾å¤‡)
  python demo_sdk.py --mode vad --device 2

  # Manual æ¨¡å¼ (éŸ³é¢‘æ–‡ä»¶)
  python demo_sdk.py --mode manual --file test.wav --url ws://localhost:8080/api-ws/v1/realtime

  # æŒ‡å®šè¯­è¨€
  python demo_sdk.py --mode manual --file test.wav --language zh
        """,
    )

    parser.add_argument(
        "--mode",
        "-m",
        choices=["vad", "manual"],
        default="manual",
        help="è¯†åˆ«æ¨¡å¼: vad=å®æ—¶å½•éŸ³, manual=éŸ³é¢‘æ–‡ä»¶ (é»˜è®¤: manual)",
    )

    parser.add_argument(
        "--url", "-u", default=DEFAULT_WS_URL, help=f"WebSocket æœåŠ¡åœ°å€ (é»˜è®¤: {DEFAULT_WS_URL})"
    )

    parser.add_argument(
        "--file",
        "-f",
        help="éŸ³é¢‘æ–‡ä»¶è·¯å¾„ (Manual æ¨¡å¼å¿…éœ€, æ”¯æŒ WAV/PCM æ ¼å¼, 16kHz, 16-bit, å•å£°é“)",
    )

    parser.add_argument(
        "--language",
        "-l",
        default="auto",
        choices=["auto", "zh", "en", "ja", "ko"],
        help="è¯†åˆ«è¯­è¨€ (é»˜è®¤: auto)",
    )

    parser.add_argument(
        "--delay", "-d", type=float, default=0.1, help="éŸ³é¢‘å‘é€é—´éš”, ç§’ (é»˜è®¤: 0.1)"
    )

    parser.add_argument(
        "--api-key", "-k", default=DEFAULT_API_KEY, help="API Key (æœ¬åœ°æœåŠ¡å¯éšæ„å¡«å†™)"
    )

    parser.add_argument(
        "--list-devices",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„éŸ³é¢‘è¾“å…¥è®¾å¤‡",
    )

    parser.add_argument(
        "--device",
        type=int,
        default=None,
        help="éŸ³é¢‘è¾“å…¥è®¾å¤‡ç´¢å¼• (ä½¿ç”¨ --list-devices æŸ¥çœ‹å¯ç”¨è®¾å¤‡)",
    )

    args = parser.parse_args()

    # åˆ—å‡ºè®¾å¤‡æ¨¡å¼
    if args.list_devices:
        list_audio_devices()
        return

    # è®¾ç½®æ—¥å¿—
    setup_logging()

    # è®¾ç½® API Key
    dashscope.api_key = args.api_key

    # è¿è¡Œ
    if args.mode == "vad":
        run_vad_mode(args.url, args.api_key, args.language, args.device)
    else:
        if not args.file:
            print("âŒ Manual æ¨¡å¼éœ€è¦æŒ‡å®š --file å‚æ•°")
            parser.print_help()
            sys.exit(1)
        run_manual_mode(args.url, args.api_key, args.file, args.language, args.delay)


if __name__ == "__main__":
    main()
