# Qwen3-ASR-Realtime Server Configuration
# vLLM Streaming Backend Only

# Server Settings
SERVER_HOST=0.0.0.0
SERVER_PORT=8080

# Qwen3-ASR Model Settings (vLLM Backend Required for Streaming)
# Supports: HF model ID (e.g., Qwen/Qwen3-ASR-1.7B), absolute path, or relative path
QWEN3_ASR_MODEL_PATH=Qwen/Qwen3-ASR-1.7B

# vLLM Settings
# GPU memory utilization (0.0 - 1.0)
GPU_MEMORY_UTILIZATION=0.8

# Max new tokens per inference step (smaller = faster streaming)
# Default 64, can increase for longer utterances
MAX_NEW_TOKENS=64

# Model dtype: auto, float16, bfloat16, float32
MODEL_DTYPE=auto

# VAD Settings
VAD_ENABLED=true
VAD_THRESHOLD=0.5
VAD_SILENCE_DURATION_MS=400

# Streaming Settings
# Audio chunk size in seconds (2.0 recommended)
STREAMING_CHUNK_SIZE_SEC=2.0

# Auto commit interval in seconds (for long audio sessions)
AUTO_COMMIT_INTERVAL_SEC=60.0

# Logging
LOG_LEVEL=info
