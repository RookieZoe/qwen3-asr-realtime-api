# Qwen3-ASR-Realtime Server Configuration
# vLLM Streaming Backend Only

# Server Settings
SERVER_HOST=0.0.0.0
SERVER_PORT=8000

# Qwen3-ASR Model Settings (vLLM Backend Required for Streaming)
QWEN3_ASR_MODEL_PATH=/path/to/your/Qwen3-ASR-model

# vLLM Settings
# GPU memory utilization (0.0 - 1.0)
GPU_MEMORY_UTILIZATION=0.8

# Max new tokens per inference step (smaller = faster streaming)
# Default 64, can increase for longer utterances
MAX_NEW_TOKENS=64

# Model dtype: auto, float16, bfloat16, float32
MODEL_DTYPE=auto

# VAD Settings
VAD_ENABLED=true
VAD_THRESHOLD=0.5
VAD_SILENCE_DURATION_MS=400

# Audio Settings
DEFAULT_SAMPLE_RATE=16000
DEFAULT_AUDIO_FORMAT=pcm

# Streaming Settings
# Audio chunk size in seconds (2.0 recommended)
STREAMING_CHUNK_SIZE_SEC=2.0

# Logging
LOG_LEVEL=info
